{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a1a889-9577-45a0-a09f-ecc68dc53f13",
   "metadata": {},
   "source": [
    "# Small local agents\n",
    "\n",
    "Experimenting with agents does not require access to external services, expensive hardware, or even the cloud.\n",
    "This short tutorial notebook is a introduction to language model-based agents using small language models, and demonstrating their use to query a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc957f3c-c960-48bd-b968-4016c4e6573d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Local Ollama server\n",
    "\n",
    "The language models will we use will be served by Ollama.\n",
    "The default instructions use `podman` to run that Ollama server in a rootless container.\n",
    "\n",
    "This assumes that `podman` is installed. If not yet the case, the instructions are here:\n",
    "https://podman.io/docs/installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd43c24-8623-44c4-9ce5-3d425587bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19a7f33e9100e1f667f371d6e5423223cdef8c889a2400943b5e7b78f5cdb707\n"
     ]
    }
   ],
   "source": [
    "! podman run -d --rm -p 11434:11434 -v dot_ollama:/root/.ollama --name ollama docker.io/alpine/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26791330-65d9-4d2b-a858-4e8499b26b13",
   "metadata": {},
   "source": [
    "## Required Python packages\n",
    "\n",
    "The Python packages `ollama` and `langchain`, and `langchain_ollama` are required. Make sure they are installed before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55bca2f-d3d2-4f5c-970d-112aa5c79698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import langchain\n",
    "import langchain_ollama\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ef193-f8cc-4b25-8637-eadc9a1e887a",
   "metadata": {},
   "source": [
    "## Utility code\n",
    "\n",
    "In order to minimize code duplication, we encapsulate most of Ollama model management into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec59247-f9ab-449a-80e3-bfa495233b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class OllamaModel:\n",
    "    \"\"\"Wrapper class for models with Ollama and LangChain.\"\"\"\n",
    "\n",
    "    name: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def has_tools(self) -> bool:\n",
    "        return 'tools' in self.show().capabilities\n",
    "\n",
    "    def is_pulled(self) -> bool:\n",
    "        for mdl in ollama.list().models:\n",
    "            if mdl.model == self.name:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Pull the model to make it available to the local Ollama instance.\"\"\"\n",
    "        ollama.pull(self.name)\n",
    "\n",
    "    def show(self) -> ollama._types.ShowResponse:\n",
    "        return ollama.show(self.name)\n",
    "\n",
    "    def new_chat(self, temperature:float = 0) -> langchain_ollama.ChatOllama:\n",
    "        res = langchain_ollama.ChatOllama(\n",
    "            model=str(self),\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db991672-d2fe-4083-b16a-6aa40a654696",
   "metadata": {},
   "source": [
    "### Object display customization\n",
    "We also create a convenience custom HTML render for AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951f8728-ae93-4316-8585-35f6ef2f10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "_aimessage_template = jinja2.Template(    \n",
    "    \"\"\"<table>\n",
    "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "      {% if response.content %}\n",
    "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
    "      <td style=\"text-align: left; text-indent: 0px;\">\n",
    "        <div style=\"white-space: pre-wrap;\">\n",
    "        {{ response.content }}\n",
    "        </div>\n",
    "      </td>\n",
    "      {% endif %}\n",
    "      {% if response.tool_calls %}\n",
    "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
    "      <td style=\"text-align: left; text-indent: 0px;\">\n",
    "        <div style=\"white-space: pre-wrap;\">\n",
    "        {{ response.tool_calls }}\n",
    "        </div>\n",
    "      </td>\n",
    "      {% endif %}\n",
    "  </tr></tbody></table>\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "def aimessage_html(obj):\n",
    "    return _aimessage_template.render(response=obj)\n",
    "    \n",
    "html_formatter = get_ipython().display_formatter.formatters['text/html']\n",
    "html_formatter.for_type_by_name('langchain_core.messages.ai', 'AIMessage', aimessage_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79636a44-b046-4c57-950d-05b64388d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toolmessage_html(obj):\n",
    "    _toolmessage_template = jinja2.Template(    \n",
    "        \"\"\"<table>\n",
    "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
    "        <tbody>\n",
    "        <tr>\n",
    "          {% if response.content %}\n",
    "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
    "          <td style=\"text-align: left; text-indent: 0px;\">\n",
    "            <div style=\"white-space: pre-wrap;\">\n",
    "            {{ response.content }}\n",
    "            </div>\n",
    "          </td>\n",
    "          {% endif %}\n",
    "        </tr>\n",
    "        </tbody></table>\n",
    "        \"\"\")\n",
    "    return _toolmessage_template.render(response=obj)\n",
    "\n",
    "\n",
    "html_formatter = get_ipython().display_formatter.formatters['text/html']\n",
    "html_formatter.for_type_by_name('langchain_core.messages.tool', 'ToolMessage', toolmessage_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87415bec-b503-4a28-a213-988649616be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(text):\n",
    "    return f'\\n\\033[1m{text}\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa916c6-07c8-4748-a122-64609cc1d6c2",
   "metadata": {},
   "source": [
    "## List of language models\n",
    "\n",
    "Below is the list all models you want to use here. Be mindful about the capabilities of the system this will run on, in particular the RAM or the VRAM is a GPU is present.\n",
    "\n",
    "This notebook was created with `gemma3:4b` (Gemma3 with 4B parameters) and `granite4:1b` (Granite4 with 1B parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ca17e5-45dc-4e12-9ebc-7f09faa4ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = (\n",
    "    # OllamaModel('deepseek-r1:1.5b'),\n",
    "    OllamaModel('gemma3:4b'),\n",
    "    OllamaModel('granite4:1b'),\n",
    "    # OllamaModel('phi4-mini:3.8b'),\n",
    "    # OllamaModel('qwen3:4b')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602365d4-f122-4207-8d8b-156bfeb611f6",
   "metadata": {},
   "source": [
    "We are ready to start. First, we ensure that the models we need are pulled locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986e0fb7-43d0-4e00-aacb-9b550f42eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gemma3:4b already pulled.\n",
      "Model granite4:1b already pulled.\n"
     ]
    }
   ],
   "source": [
    "for mdl in MODELS:\n",
    "    if mdl.is_pulled():\n",
    "        print(f'Model {mdl} already pulled.')\n",
    "    else:\n",
    "        print(f'Pulling model {mdl}...', end='', flush=True)\n",
    "        mdl.pull()\n",
    "        print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f24dbe-7eb2-4b47-97a9-e25db71eaa80",
   "metadata": {},
   "source": [
    "# Language models and their limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1baa31f-f3d5-4dfc-a001-ec00e7a09fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f142b315-5190-47df-93ea-9e9d66ea94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        Okay, let's calculate 392373366237 * 562310951975.\n",
       "\n",
       "This is a large multiplication, and it's best done with a calculator or a programming language.  I'll use Python for this calculation:\n",
       "\n",
       "```python\n",
       "num1 = 392373366237\n",
       "num2 = 562310951975\n",
       "\n",
       "result = num1 * num2\n",
       "print(result)\n",
       "```\n",
       "\n",
       "This code will output:\n",
       "\n",
       "2039999999999999999999999999999999\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content=\"Okay, let's calculate 392373366237 * 562310951975.\\n\\nThis is a large multiplication, and it's best done with a calculator or a programming language.  I'll use Python for this calculation:\\n\\n```python\\nnum1 = 392373366237\\nnum2 = 562310951975\\n\\nresult = num1 * num2\\nprint(result)\\n```\\n\\nThis code will output:\\n\\n2039999999999999999999999999999999\", additional_kwargs={}, response_metadata={}, id='lc_run--019aff79-faa6-7cd0-8dd5-32dc773eabdd-0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The result of multiplying 392373366237 by 562310951975 is:\n",
       "\n",
       "220,646,204,278,964,000,795\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The result of multiplying 392373366237 by 562310951975 is:\\n\\n220,646,204,278,964,000,795', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:40:50.682884839Z', 'done': True, 'done_reason': 'stop', 'total_duration': 41723327372, 'load_duration': 3010227073, 'prompt_eval_count': 46, 'prompt_eval_duration': 18709831723, 'eval_count': 31, 'eval_duration': 19975427110, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7a-7d7b-7f71-a5f4-00c2bfe87062-0', usage_metadata={'input_tokens': 46, 'output_tokens': 31, 'total_tokens': 77})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage('Make the following calculation: 392373366237 * 562310951975')\n",
    "for mdl in MODELS:\n",
    "    print(f'\\n\\033[1m{str(mdl)}\\033[0m\\n')\n",
    "    chat = mdl.new_chat()\n",
    "    response = chat.invoke([human_message])\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524db586-53bb-4ec6-a706-9d6a6f19ae7c",
   "metadata": {},
   "source": [
    "\n",
    "Is this correct? The actual answer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55080cca-304b-4e1e-96ae-a8864c04fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220,635,841,098,362,793,468,075\n"
     ]
    }
   ],
   "source": [
    "print(f'{392373366237 * 562310951975:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501905b7-ff51-4e88-b8d8-9f99c7735f9d",
   "metadata": {},
   "source": [
    "Unless this notebook is running in a future when LLMs have learned to multiply, or this notebook became part of the training data, the LLM answer is likely not correct.\n",
    "\n",
    "The point to remember is to not rely on the current LLM technology to perform correctly mathematical operations, or more generally to evaluate expressions or run algorithms. However, they might be able to generate code (software instructions), for example a function call, to get to the answer. This can be observed when using `gemma3:4b`: the answer from the LLM shows includes Python code that returns the correct answer is evaluated.\n",
    "\n",
    "If we have a way to know when LLM answers contains code, we could simply evaluate that code and consider the resulting text its answer a dialogue. In fact, it does not even have to be a dialogue and code evaluation represent simply an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ace3e-23a3-4407-bb6f-72ef29840109",
   "metadata": {},
   "source": [
    "Giving additional instructions in the prompt can get us there with this example. We can add that if the pattern matching in the LLMs considers that evaluating code would be a better path to the correct answer, only code should be returned. To facilitate the identification of such answer we also indicate the format to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e271f4-d916-4e5d-a5b2-c56e8f50d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```python\n",
       "392373366237 * 562310951975\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```python\\n392373366237 * 562310951975\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:40:58.561632809Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7847118974, 'load_duration': 277482052, 'prompt_eval_count': 83, 'prompt_eval_duration': 1363682037, 'eval_count': 32, 'eval_duration': 6120723632, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7b-2099-75a0-bc6b-96a1a245b304-0', usage_metadata={'input_tokens': 83, 'output_tokens': 32, 'total_tokens': 115})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```python\n",
       "result = 392373366237 * 562310951975\n",
       "result\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```python\\nresult = 392373366237 * 562310951975\\nresult\\n```', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:41:13.720214429Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15130838249, 'load_duration': 124583662, 'prompt_eval_count': 86, 'prompt_eval_duration': 11476726492, 'eval_count': 21, 'eval_duration': 3507454484, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7b-3f5b-77e2-95bb-182ac411f156-0', usage_metadata={'input_tokens': 86, 'output_tokens': 21, 'total_tokens': 107})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_as_code = HumanMessage(\"\"\"\n",
    "IF this type of question would be better answered by evaluating code, return instead\n",
    "ONLY Python code in the answer. Use the Markdown format for code snippets:\n",
    "```python\n",
    "<code>\n",
    "```\n",
    "\"\"\")\n",
    "human_message = HumanMessage(\"\"\"Make the following calculation: 392373366237 * 562310951975\"\"\")\n",
    "for mdl in MODELS:\n",
    "    print(highlight(str(mdl)))\n",
    "    chat = mdl.new_chat()\n",
    "    response = chat.invoke([human_message, answer_as_code])\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ccb5c-bca6-4f9c-be94-7948dd4f95a4",
   "metadata": {},
   "source": [
    "We check our new prompt giving the option to answer with code with a question that should be answerable without code.\n",
    "That is a question for which the answer was almost certainly in the training data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f981aa2-9a7d-400b-94b2-97830d24d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```python\n",
       "print(\"Moon\")\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```python\\nprint(\"Moon\")\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:41:17.283402346Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3537444852, 'load_duration': 318737561, 'prompt_eval_count': 64, 'prompt_eval_duration': 1770844038, 'eval_count': 10, 'eval_duration': 1421325110, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7b-7a90-76a1-9cd5-15358a03b683-0', usage_metadata={'input_tokens': 64, 'output_tokens': 10, 'total_tokens': 74})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The name of the natural satellite orbiting around the Earth is called **the Moon**.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The name of the natural satellite orbiting around the Earth is called **the Moon**.', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:41:30.022761765Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12713884135, 'load_duration': 110351251, 'prompt_eval_count': 83, 'prompt_eval_duration': 8633197229, 'eval_count': 19, 'eval_duration': 3948868046, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7b-887a-7cb1-a5e9-4558ab6c99eb-0', usage_metadata={'input_tokens': 83, 'output_tokens': 19, 'total_tokens': 102})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage(\"\"\"What is the name of the natural satellite orbiting around the earth?\"\"\")\n",
    "for mdl in MODELS:\n",
    "    print(highlight(str(mdl)))\n",
    "    chat = mdl.new_chat()\n",
    "    response = chat.invoke([human_message, answer_as_code])\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba2038-fe69-4e53-8126-0d748129887e",
   "metadata": {},
   "source": [
    "While `gemma3:4b` its technically correct, it may indicate that this model will resort to writing code more often than necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae76940-5457-4ecf-818d-c67cf0f4627b",
   "metadata": {},
   "source": [
    "# Adding tools as context\n",
    "\n",
    "Agents are language models to which the availability of tools (software functions), with a description of what they do, is specified as context. For `gemma3:4b` this can be achieved through a system prompt, while a few other models with a \"tools\" capability Python functions can passed directly. This is the case with `granite4:1b` for example.\n",
    "\n",
    "## Tools in prompt\n",
    "\n",
    "We showed earlier that a prompt can indicate that code evaluation is available.\n",
    "Similarly, we can indicate that tools are available and calling a tool can be used as an answer.\n",
    "\n",
    "If we make our multiplication a tool, a prompt can look like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d8a6648-b39f-41fd-9a8d-6e1ae5a9f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_tools = \"\"\"\n",
    "You have access to functions. \n",
    "You don't have to use tools if not necessary, but *IF* you decide to invoke any of the function(s),\n",
    "you MUST put it in a valid JSON expression like:\n",
    "[\n",
    "  {\"name\": function name,\n",
    "   \"parameters\": dictionary of argument name and its value}\n",
    "]\n",
    "\n",
    "You SHOULD NOT include any other text in the response if you call a function.\n",
    "\n",
    "Functions:\n",
    "[\n",
    "  { \"name\": \"multiply\",\n",
    "    \"description\": \"Multiply two numbers.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "          \"a\": \"number\",\n",
    "          \"b\": \"number\"\n",
    "      },\n",
    "      \"required\": [\n",
    "          \"a\", \"b\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1fe2c-82ea-4845-bcb3-da20e46502f6",
   "metadata": {},
   "source": [
    "## Tools as `tools`\n",
    "\n",
    "The use of tools has been somewhat formalized since the early days of prompt engineering, a mere double-digit number\n",
    "of months in the past, we have seen so far.\n",
    "\n",
    "Many of the most recent language model specifically include a capability called \"tools\". This frees a user, or developer,\n",
    "from having to work on prompt sections that declare the availablity of tools in a way that the language model reacts to it, and on parsing out if an answer is text or code. `langchain` provides a wrapper for this and let one simply declare any Python function as tool. For the multiplication we can just plug the Python multiplication operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d06f06-7e0b-44c8-a003-a28e8e08e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "tools = [tool(operator.mul)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1b152-e72d-411e-83dc-9621bc585bfc",
   "metadata": {},
   "source": [
    "## Language model opting to answer with a tool call\n",
    "\n",
    "We have shown that a prompt context, wrapped in a `langchain` tool declaration when the model has the capability, can lead a language model to answer with code. If we ask again our multiplication question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0780b2-8460-40fc-a241-65a98db4b22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"multiply\",\n",
       "    \"parameters\": {\n",
       "      \"a\": 392373366237,\n",
       "      \"b\": 562310951975\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"multiply\",\\n    \"parameters\": {\\n      \"a\": 392373366237,\\n      \"b\": 562310951975\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:42:02.454446058Z', 'done': True, 'done_reason': 'stop', 'total_duration': 32194077932, 'load_duration': 335894417, 'prompt_eval_count': 224, 'prompt_eval_duration': 5376817252, 'eval_count': 69, 'eval_duration': 26433051974, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7b-bb11-7e61-82c4-b89d7aef939e-0', usage_metadata={'input_tokens': 224, 'output_tokens': 69, 'total_tokens': 293})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'mul', 'args': {'a': '392373366237', 'b': '562310951975'}, 'id': 'e5b6cb9c-bb6e-42ac-86a6-aff3fc9dda05', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:42:18.42391671Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15884430814, 'load_duration': 89324640, 'prompt_eval_count': 199, 'prompt_eval_duration': 11520595603, 'eval_count': 31, 'eval_duration': 4243729556, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7c-3929-7f01-ae3c-d46f83b4d00a-0', tool_calls=[{'name': 'mul', 'args': {'a': '392373366237', 'b': '562310951975'}, 'id': 'e5b6cb9c-bb6e-42ac-86a6-aff3fc9dda05', 'type': 'tool_call'}], usage_metadata={'input_tokens': 199, 'output_tokens': 31, 'total_tokens': 230})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage(\"\"\"Make the following calculation: 392373366237 * 562310951975\"\"\")\n",
    "\n",
    "for model in MODELS:\n",
    "    chat = model.new_chat()\n",
    "    if model.has_tools():\n",
    "        print(f'{highlight(model.name)} (with tools)\\n')\n",
    "        chat = chat.bind_tools(tools)\n",
    "        messages = [human_message]\n",
    "    else:\n",
    "        print(f'{highlight(model.name)}\\n')\n",
    "        messages = [SystemMessage(system_prompt_tools),\n",
    "                    human_message]\n",
    "    response = chat.invoke(messages)\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33783eec-2d72-48f6-ba3c-6d8acf17f0a0",
   "metadata": {},
   "source": [
    "# Agentic linear chaining - an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd37e72-5ce8-42c6-bd45-078fb67f220c",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707c233c-6cd6-440c-8f62-3d42dd619f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "dbcon = sqlite3.connect(':memory:', check_same_thread=False)\n",
    "dbcon.execute(\"\"\"\n",
    "CREATE TABLE medication(\n",
    "ndc STRING PRIMARY KEY,\n",
    "common_name STRING,\n",
    "quantity INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "for values in (\n",
    "    ('ABCD-DEF', 'Aspirin', 3), ('ABCD-GHI', 'Aspirin', 5), ('GHIK-KLM', 'bacitracin', 10)\n",
    "):\n",
    "    dbcon.execute(\"\"\"\n",
    "    INSERT INTO medication(ndc, common_name, quantity) VALUES (?,?,?);\n",
    "    \"\"\", values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd787f-11f1-4a3b-b75b-5ca7a86712d0",
   "metadata": {},
   "source": [
    "Functions to query the database:\n",
    "- `get_ndc_code` to get all NDC codes for the common name of a medication\n",
    "- `get_medication_stock` to get the stock for a given NDC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fd751c7-b3aa-4510-baee-b14a523013db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndc_code(medication: str) -> tuple[str, ...]:\n",
    "    \"\"\"Get possible ndc codes for a medication.\n",
    "    \n",
    "    Args:\n",
    "      medication: name of the medication for which NDC code should be returned.\n",
    "    \"\"\"\n",
    "    cursor = dbcon.cursor()\n",
    "    cursor.execute('SELECT ndc FROM medication WHERE common_name==?',\n",
    "                   (medication, ))\n",
    "    return tuple(row[0] for row in cursor.fetchall())\n",
    "\n",
    "\n",
    "def get_ndc_stock(ndc_code: str) -> int:\n",
    "    \"\"\"Get the number of boxes of medication in stock.\n",
    "    \n",
    "    Args:\n",
    "      ndc_code: NDC code to retrieve stock for.\n",
    "    \"\"\"\n",
    "    cursor = dbcon.cursor()\n",
    "    cursor.execute('SELECT quantity FROM medication WHERE ndc==?',\n",
    "                   (ndc_code, ))\n",
    "    return cursor.fetchone()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c806bb0-ef29-404e-975d-b65bcd2a60f4",
   "metadata": {},
   "source": [
    "The prompt may contain several tools available. A prompt context will be similar to the one we showed for the multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2514d9eb-f292-4771-9db9-10c6558ca1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_tools = \"\"\"\n",
    "You have access to functions. \n",
    "You don't have to use tools if not necessary, but *IF* you decide to invoke any of the function(s),\n",
    "you MUST put it in a valid JSON expression like:\n",
    "[\n",
    "  {\"name\": function name,\n",
    "   \"parameters\": dictionary of argument name and its value}\n",
    "]\n",
    "\n",
    "You SHOULD NOT include any other text in the response if you call a function.\n",
    "\n",
    "Functions:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"get_ndc_stock\",\n",
    "    \"description\": \"Get the number of boxes of medication in stock.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"ndc_code\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"ndc_code\"\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"get_ndc_code\",\n",
    "    \"description\": \"Get possible ndc codes for a medication.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"medication\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"medication\"\n",
    "      ]\n",
    "    },\n",
    "  }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003ffb3-7718-440b-be9a-e15c67751761",
   "metadata": {},
   "source": [
    "Whenever a model has the capabilities \"tools\", the user or developer is spared the prompt context engineering.\n",
    "The decorator `tool()` in `langchain` can be used instead.\n",
    "\n",
    "Note: `langchain` is essentially an abstraction layer over prompts.\n",
    "It can create the equivalent of our engineered prompt contexts from Python function definitions,\n",
    "feeding the LLM with information in the docstring, or in additional declarative structures\n",
    "(see https://docs.langchain.com/oss/python/langchain/tools#advanced-schema-definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b8408f1-7664-4703-85c0-0b07099af717",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool(get_ndc_code), tool(get_ndc_stock)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8606c95c-9e26-418e-b3cd-bcd93886cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        January! ðŸ˜Š \n",
       "\n",
       "Do you want to play a quick game of guessing games?\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='January! ðŸ˜Š \\n\\nDo you want to play a quick game of guessing games?', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:42:24.94549934Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6288765459, 'load_duration': 279177011, 'prompt_eval_count': 22, 'prompt_eval_duration': 628894393, 'eval_count': 18, 'eval_duration': 5363950493, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7c-781f-7b53-b224-0d6959735615-0', usage_metadata={'input_tokens': 22, 'output_tokens': 18, 'total_tokens': 40})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The first month of the year is **January**.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The first month of the year is **January**.', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:42:42.398600321Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17347730278, 'load_duration': 82920434, 'prompt_eval_count': 268, 'prompt_eval_duration': 15096003132, 'eval_count': 12, 'eval_duration': 2147303290, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7c-9118-70e2-bf33-034673ccaf83-0', usage_metadata={'input_tokens': 268, 'output_tokens': 12, 'total_tokens': 280})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def invoke_model(model, human_message, system_message=SystemMessage('')):\n",
    "    chat = model.new_chat()\n",
    "    if model.has_tools():\n",
    "        print(f'\\n{highlight(model.name)} (with tools)\\n')\n",
    "        chat = chat.bind_tools(tools)\n",
    "    else:\n",
    "        print(f'\\n{highlight(model.name)}\\n')\n",
    "    messages = [system_message,\n",
    "                human_message]\n",
    "    response = chat.invoke(messages)\n",
    "    return response, chat\n",
    "\n",
    "human_message = HumanMessage('Name the first month of the year.')\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message)\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaac153b-042f-49b7-b6a1-4dca1f1a897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_map = {tool.name: tool for tool in tools}\n",
    "if len(tool_map) != len(tools):\n",
    "    raise ValueError(\"'tools' contains at lease one tool name duplicate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b89ee6be-12f2-446a-a92e-ff451dee26da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"get_ndc_stock\",\n",
       "    \"parameters\": {\n",
       "      \"ndc_code\": \"ABCD-DEF\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"get_ndc_stock\",\\n    \"parameters\": {\\n      \"ndc_code\": \"ABCD-DEF\"\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:43:14.71736512Z', 'done': True, 'done_reason': 'stop', 'total_duration': 32130731345, 'load_duration': 293680086, 'prompt_eval_count': 318, 'prompt_eval_duration': 7484939469, 'eval_count': 50, 'eval_duration': 24301447787, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7c-d599-72f2-a604-e80b3480a772-0', usage_metadata={'input_tokens': 318, 'output_tokens': 50, 'total_tokens': 368})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing automatically if there is a tool call is left as an exercise for the reader.\n",
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': '2b26efbe-2735-4f51-89dc-2f2f6647b2ff', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:43:37.034360037Z', 'done': True, 'done_reason': 'stop', 'total_duration': 22132885904, 'load_duration': 196241314, 'prompt_eval_count': 518, 'prompt_eval_duration': 16952269961, 'eval_count': 27, 'eval_duration': 4939081527, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7d-53c9-7881-bf4f-5e6fef94f55a-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': '2b26efbe-2735-4f51-89dc-2f2f6647b2ff', 'type': 'tool_call'}], usage_metadata={'input_tokens': 518, 'output_tokens': 27, 'total_tokens': 545})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> calling get_ndc_stock(ndc_code='ABCD-DEF')\n",
      "      result:\n",
      "        3\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage('How many boxes of medication with NDC code ABCD-DEF do we have left?')\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message, SystemMessage(system_prompt_tools))\n",
    "    display(response)\n",
    "    if response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            print(\n",
    "                f'  --> calling {tool_call[\"name\"]}'\n",
    "                f'({\", \".join(\"=\".join((k, repr(v))) for k, v in tool_call[\"args\"].items())})'\n",
    "            )\n",
    "            selected_tool = tool_map[tool_call['name']]\n",
    "            res = selected_tool.invoke(tool_call)\n",
    "            print('      result:')\n",
    "            print(f'        {res.content}')\n",
    "    else:\n",
    "        print('Assessing automatically if there is a tool call is left as an exercise for the reader.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d949565-bb5d-4e3a-a70b-2e5b2ef8d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"get_ndc_code\",\n",
       "    \"parameters\": {\n",
       "      \"medication\": \"Aspirin\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"get_ndc_code\",\\n    \"parameters\": {\\n      \"medication\": \"Aspirin\"\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:43:47.740582126Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10545446906, 'load_duration': 296672173, 'prompt_eval_count': 315, 'prompt_eval_duration': 1035159792, 'eval_count': 48, 'eval_duration': 9154730895, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7d-aae9-7b73-95c0-1deef2a64f24-0', usage_metadata={'input_tokens': 315, 'output_tokens': 48, 'total_tokens': 363})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing automatically if there is a tool call is left as an exercise for the reader.\n",
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': 'c072a779-c583-4add-b4e2-0dad061774a1', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:44:02.318015134Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14463488728, 'load_duration': 133969511, 'prompt_eval_count': 514, 'prompt_eval_duration': 9727646180, 'eval_count': 25, 'eval_duration': 4568147270, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7d-d48c-7113-8c5b-4ada27a19d71-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': 'c072a779-c583-4add-b4e2-0dad061774a1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 514, 'output_tokens': 25, 'total_tokens': 539})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      result:\n",
      "        [\"ABCD-DEF\", \"ABCD-GHI\"]\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(\"What are possible NDC codes for the medication 'Aspirin'?\")\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message, SystemMessage(system_prompt_tools))\n",
    "    display(response)\n",
    "    if response.tool_calls:\n",
    "        print('      result:')\n",
    "        for tool_call in response.tool_calls:\n",
    "            selected_tool = tool_map[tool_call['name']]\n",
    "            res = selected_tool.invoke(tool_call)    \n",
    "            print(f'        {res.content}')\n",
    "    else:\n",
    "        print('Assessing automatically if there is a tool call is left as an exercise for the reader.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e68e5-a09f-4778-915d-e206dc823708",
   "metadata": {},
   "source": [
    "Cool. Uut what did we exactly gain here? If you are are programmer or a data analyst knowing how to write SQL, arguably not much. However, what we have achieved is akin to a text user interface, or language user interface, that can be an alternative to a graphical user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0e438-ae36-40ef-bc90-d81d2ba431bf",
   "metadata": {},
   "source": [
    "## Reasoning in steps\n",
    "\n",
    "So far we only showed questions for which either the language model had the answer memorized from the data it was trained on, or for which the answer was one tool call away.\n",
    "\n",
    "There are questions questions for which the answer requires a sequence of recall from training data or tool calls, with the output of a step used as the input to the next step. For example, asking for how many Aspirin boxes are in stock, would require to 1) get the NDC codes for Aspirin products, 2) query the stock for each, and finally 3) sum those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b26b128-758a-4a14-872a-f222dfc4b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"get_ndc_code\",\n",
       "    \"parameters\": {\n",
       "      \"medication\": \"Aspirin\"\n",
       "    }\n",
       "  },\n",
       "  {\n",
       "    \"name\": \"get_ndc_stock\",\n",
       "    \"parameters\": {\n",
       "      \"ndc_code\": \"0003499\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"get_ndc_code\",\\n    \"parameters\": {\\n      \"medication\": \"Aspirin\"\\n    }\\n  },\\n  {\\n    \"name\": \"get_ndc_stock\",\\n    \"parameters\": {\\n      \"ndc_code\": \"0003499\"\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-08T19:44:31.516590916Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29078289523, 'load_duration': 321380361, 'prompt_eval_count': 315, 'prompt_eval_duration': 744411909, 'eval_count': 93, 'eval_duration': 27906190738, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019aff7e-0d84-7142-a6ec-c5960b68aef4-0', usage_metadata={'input_tokens': 315, 'output_tokens': 93, 'total_tokens': 408})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '2d0c4128-8708-407d-9f35-12332528adc6', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:44:44.407637549Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12814503591, 'load_duration': 97660305, 'prompt_eval_count': 514, 'prompt_eval_duration': 9037216110, 'eval_count': 25, 'eval_duration': 3647147547, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7e-7f67-7ed0-aed5-6bc8f2d37a53-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '2d0c4128-8708-407d-9f35-12332528adc6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 514, 'output_tokens': 25, 'total_tokens': 539})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage('How many boxes of Aspirin do we have in stock altogether?')\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message, SystemMessage(system_prompt_tools))\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e3230-52f6-4f48-a71e-e1fdc745a1ce",
   "metadata": {},
   "source": [
    "Odds are are that the results are not great. Bare language models used in this notebook generally do not handle this well without guidance from a prompt. For example,  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199acc6-4d4e-4f41-b8f3-1c9fd11c6695",
   "metadata": {},
   "source": [
    "## ReAct\n",
    "\n",
    "The ReAct approach prompts a language model to match its answers to the following pattern:\n",
    "1. **question**\n",
    "2. **thought:** the language model's answer to the question. The answer can be the use of a tool.\n",
    "3. **action:** whenever the answer is the use of a tool, a function call, this is selected here.\n",
    "4. **action input:** the input to the action.\n",
    "5. **observation:** observe the result of performing the action.\n",
    "6. The sequence of steps 2 to 5 can be repeated N times\n",
    "7. **final answer**\n",
    "\n",
    "It is derived from the empirical observation that LLMs appeared to give more \"thoughtful\" answers, or better solve reasoning-oriented questions when asked to think step by step.\n",
    "\n",
    "React can be idependently achieved through adding the description to a prompt and iteratively parsing language model responses and crafting new messages. However, in many scenario this is can be abstracted out through the use of library. We use `langchain` again to demonstrate how it can work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f87632a-ccf4-444c-b1e3-f3ca03557058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "Parsing AI messages into next actions and results is left as an exercise.\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '5431dcce-5fd8-439c-bddc-5eee0c2ce893', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:44:55.966593969Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11319965889, 'load_duration': 92762577, 'prompt_eval_count': 274, 'prompt_eval_duration': 4862770074, 'eval_count': 25, 'eval_duration': 6339518881, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7e-b265-7b62-9b54-136424e513a7-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '5431dcce-5fd8-439c-bddc-5eee0c2ce893', 'type': 'tool_call'}], usage_metadata={'input_tokens': 274, 'output_tokens': 25, 'total_tokens': 299})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
       "        <tbody>\n",
       "        <tr>\n",
       "          \n",
       "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "          <td style=\"text-align: left; text-indent: 0px;\">\n",
       "            <div style=\"white-space: pre-wrap;\">\n",
       "            [\"ABCD-DEF\", \"ABCD-GHI\"]\n",
       "            </div>\n",
       "          </td>\n",
       "          \n",
       "        </tr>\n",
       "        </tbody></table>\n",
       "        "
      ],
      "text/plain": [
       "ToolMessage(content='[\"ABCD-DEF\", \"ABCD-GHI\"]', name='get_ndc_code', id='a6d16352-6aec-4662-be37-1e067b9ee01a', tool_call_id='5431dcce-5fd8-439c-bddc-5eee0c2ce893')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': '809d80b5-82ca-4e67-b38c-8ef1f8f8f5ed', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:45:01.73438538Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5752588051, 'load_duration': 50907407, 'prompt_eval_count': 323, 'prompt_eval_duration': 1113221688, 'eval_count': 27, 'eval_duration': 4562128881, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7e-deac-7ed1-9c16-fbe4db80ac77-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': '809d80b5-82ca-4e67-b38c-8ef1f8f8f5ed', 'type': 'tool_call'}], usage_metadata={'input_tokens': 323, 'output_tokens': 27, 'total_tokens': 350})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
       "        <tbody>\n",
       "        <tr>\n",
       "          \n",
       "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "          <td style=\"text-align: left; text-indent: 0px;\">\n",
       "            <div style=\"white-space: pre-wrap;\">\n",
       "            3\n",
       "            </div>\n",
       "          </td>\n",
       "          \n",
       "        </tr>\n",
       "        </tbody></table>\n",
       "        "
      ],
      "text/plain": [
       "ToolMessage(content='3', name='get_ndc_stock', id='cef8e30b-29a4-4f95-94e7-58c7281ca124', tool_call_id='809d80b5-82ca-4e67-b38c-8ef1f8f8f5ed')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-GHI'}, 'id': 'd79c5738-7b77-4aa9-9dac-860211704613', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:45:07.050301472Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5306337147, 'load_duration': 74687734, 'prompt_eval_count': 364, 'prompt_eval_duration': 721335696, 'eval_count': 26, 'eval_duration': 4481270826, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7e-f52f-7453-839f-1efbb1bed773-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-GHI'}, 'id': 'd79c5738-7b77-4aa9-9dac-860211704613', 'type': 'tool_call'}], usage_metadata={'input_tokens': 364, 'output_tokens': 26, 'total_tokens': 390})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
       "        <tbody>\n",
       "        <tr>\n",
       "          \n",
       "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "          <td style=\"text-align: left; text-indent: 0px;\">\n",
       "            <div style=\"white-space: pre-wrap;\">\n",
       "            5\n",
       "            </div>\n",
       "          </td>\n",
       "          \n",
       "        </tr>\n",
       "        </tbody></table>\n",
       "        "
      ],
      "text/plain": [
       "ToolMessage(content='5', name='get_ndc_stock', id='61e6d106-f5ac-415f-8468-d3991c30d716', tool_call_id='d79c5738-7b77-4aa9-9dac-860211704613')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The total number of boxes in stock for Aspirin is **8**.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The total number of boxes in stock for Aspirin is **8**.', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-08T19:45:10.836298484Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3772480912, 'load_duration': 69314122, 'prompt_eval_count': 405, 'prompt_eval_duration': 453153081, 'eval_count': 17, 'eval_duration': 3226539227, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019aff7f-09f5-7960-ba03-6341d65df4cf-0', usage_metadata={'input_tokens': 405, 'output_tokens': 17, 'total_tokens': 422})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "human_message = HumanMessage(\"How many boxes of Aspirin do we have in stock altogether?\")\n",
    "\n",
    "for model in MODELS:\n",
    "    print(highlight(model.name))\n",
    "    if model.has_tools():\n",
    "        react = create_agent(model.new_chat(), tools=tools)\n",
    "        for chunk in react.stream({'messages': [human_message]}, stream_mode='updates'):\n",
    "            for k in chunk.keys():\n",
    "                for m in chunk[k]['messages']:\n",
    "                    display(m)\n",
    "    else:\n",
    "        print('Parsing AI messages into next actions and results is left as an exercise.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e42bb7-e3b0-4079-b4d3-925092c86a53",
   "metadata": {},
   "source": [
    "The answer is indeed correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af9ef0-cfc6-4df9-8520-b8ad36f107da",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Small language models are able to handle simple queries and chain sequences of steps to reach an answer. They can run locally / isolated and on relatively modest hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
