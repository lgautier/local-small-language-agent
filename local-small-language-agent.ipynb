{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a1a889-9577-45a0-a09f-ecc68dc53f13",
   "metadata": {},
   "source": [
    "# Small local agents\n",
    "\n",
    "Experimenting with agents does not require access to external services, expensive hardware, or even the cloud.\n",
    "This short tutorial notebook is a introduction to language model-based agents using small language models, and demonstrating their use to query a database.\n",
    "\n",
    "This is a repository on Github:\n",
    "https://github.com/lgautier/local-small-language-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc957f3c-c960-48bd-b968-4016c4e6573d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Local Ollama server\n",
    "\n",
    "The language models will we use will be served by Ollama.\n",
    "The default instructions use `podman` to run that Ollama server in a rootless container.\n",
    "\n",
    "**If podman cannot be available, or using an isolated system where running ollama outside of\n",
    "a container is not an issue, alternative instructions follow.**\n",
    "\n",
    "This assumes that `podman` is installed. If not yet the case, the instructions are here:\n",
    "https://podman.io/docs/installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd43c24-8623-44c4-9ce5-3d425587bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed86c9e2787467f1ee8c4c1e8996388bd48bbc189d8792e8e6be7800d06ddccc\n"
     ]
    }
   ],
   "source": [
    "! podman run --replace -d --rm -p 11434:11434 -v dot_ollama:/root/.ollama --name ollama docker.io/ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3be30a-f9a2-4cc3-8412-665b29c99c2a",
   "metadata": {},
   "source": [
    "If on a debian-based system where sudo-ing an installation of ollama is possible,\n",
    "for example on Google Colab, the following instructions can be used. Create a new\n",
    "jupyter cell with the code below, or replace the podman cell above with it.\n",
    "\n",
    "```bash\n",
    "# Derived from the the Google Cloud doc:\n",
    "# https://medium.com/google-cloud/gemma-3-ollama-on-colab-a-developers-quickstart-7bbf93ab8fef\n",
    "! echo $(tput setaf 1)\"Installing APT packages.\"$(tput setaf 0)\n",
    "! sudo apt-get -qq update && sudo apt-get -qq install -y pciutils lshw\n",
    "! echo $(tput setaf 1)\"Fetching Ollama.\"$(tput setaf 0)\n",
    "! curl -fsSL https://ollama.com/install.sh | sh\n",
    "! echo $(tput setaf 1)\"Starting Ollama server.\"$(tput setaf 0)\n",
    "! nohup ollama serve > ollama.log 2>&1 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26791330-65d9-4d2b-a858-4e8499b26b13",
   "metadata": {},
   "source": [
    "## Required Python packages\n",
    "\n",
    "The Python packages `ollama` and `langchain`, and `langchain_ollama` are required. Make sure they are installed before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55bca2f-d3d2-4f5c-970d-112aa5c79698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import langchain\n",
    "import langchain_ollama\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ef193-f8cc-4b25-8637-eadc9a1e887a",
   "metadata": {},
   "source": [
    "## Utility code\n",
    "\n",
    "In order to minimize code duplication, we encapsulate most of Ollama model management into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec59247-f9ab-449a-80e3-bfa495233b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class OllamaModel:\n",
    "    \"\"\"Wrapper class for models with Ollama and LangChain.\"\"\"\n",
    "\n",
    "    name: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def has_tools(self) -> bool:\n",
    "        return 'tools' in self.show().capabilities\n",
    "\n",
    "    def is_pulled(self) -> bool:\n",
    "        for mdl in ollama.list().models:\n",
    "            if mdl.model == self.name:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Pull the model to make it available to the local Ollama instance.\"\"\"\n",
    "        ollama.pull(self.name)\n",
    "\n",
    "    def show(self) -> ollama._types.ShowResponse:\n",
    "        return ollama.show(self.name)\n",
    "\n",
    "    def new_chat(self, temperature:float = 0) -> langchain_ollama.ChatOllama:\n",
    "        res = langchain_ollama.ChatOllama(\n",
    "            model=str(self),\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db991672-d2fe-4083-b16a-6aa40a654696",
   "metadata": {},
   "source": [
    "### Object display customization\n",
    "We also create a convenience custom HTML render for AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951f8728-ae93-4316-8585-35f6ef2f10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "_aimessage_template = jinja2.Template(    \n",
    "    \"\"\"<table>\n",
    "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "      {% if response.content %}\n",
    "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
    "      <td style=\"text-align: left; text-indent: 0px;\">\n",
    "        <div style=\"white-space: pre-wrap;\">\n",
    "        {{ response.content }}\n",
    "        </div>\n",
    "      </td>\n",
    "      {% endif %}\n",
    "      {% if response.tool_calls %}\n",
    "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
    "      <td style=\"text-align: left; text-indent: 0px;\">\n",
    "        <div style=\"white-space: pre-wrap;\">\n",
    "        {{ response.tool_calls }}\n",
    "        </div>\n",
    "      </td>\n",
    "      {% endif %}\n",
    "  </tr></tbody></table>\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "def aimessage_html(obj):\n",
    "    return _aimessage_template.render(response=obj)\n",
    "    \n",
    "html_formatter = get_ipython().display_formatter.formatters['text/html']\n",
    "html_formatter.for_type_by_name('langchain_core.messages.ai', 'AIMessage', aimessage_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79636a44-b046-4c57-950d-05b64388d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toolmessage_html(obj):\n",
    "    _toolmessage_template = jinja2.Template(    \n",
    "        \"\"\"<table>\n",
    "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
    "        <tbody>\n",
    "        <tr>\n",
    "          {% if response.content %}\n",
    "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
    "          <td style=\"text-align: left; text-indent: 0px;\">\n",
    "            <div style=\"white-space: pre-wrap;\">\n",
    "            {{ response.content }}\n",
    "            </div>\n",
    "          </td>\n",
    "          {% endif %}\n",
    "        </tr>\n",
    "        </tbody></table>\n",
    "        \"\"\")\n",
    "    return _toolmessage_template.render(response=obj)\n",
    "\n",
    "\n",
    "html_formatter = get_ipython().display_formatter.formatters['text/html']\n",
    "html_formatter.for_type_by_name('langchain_core.messages.tool', 'ToolMessage', toolmessage_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87415bec-b503-4a28-a213-988649616be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(text):\n",
    "    return f'\\n\\033[1m{text}\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa916c6-07c8-4748-a122-64609cc1d6c2",
   "metadata": {},
   "source": [
    "## List of language models\n",
    "\n",
    "Below is the list all models you want to use here. Be mindful about the capabilities of the system this will run on, in particular the RAM or the VRAM is a GPU is present.\n",
    "\n",
    "This notebook was created with `gemma3:4b` (Gemma3 with 4B parameters) and `granite4:1b` (Granite4 with 1B parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ca17e5-45dc-4e12-9ebc-7f09faa4ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = (\n",
    "    # OllamaModel('deepseek-r1:1.5b'),\n",
    "    OllamaModel('gemma3:4b'),\n",
    "    OllamaModel('functiongemma:270m'),\n",
    "    OllamaModel('granite4:1b'),\n",
    "    # OllamaModel('phi4-mini:3.8b'),\n",
    "    # OllamaModel('qwen3:4b')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602365d4-f122-4207-8d8b-156bfeb611f6",
   "metadata": {},
   "source": [
    "We are ready to start. First, we ensure that the models we need are pulled locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986e0fb7-43d0-4e00-aacb-9b550f42eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gemma3:4b already pulled.\n",
      "Pulling model functiongemma:270m...done.\n",
      "Model granite4:1b already pulled.\n"
     ]
    }
   ],
   "source": [
    "for mdl in MODELS:\n",
    "    if mdl.is_pulled():\n",
    "        print(f'Model {mdl} already pulled.')\n",
    "    else:\n",
    "        print(f'Pulling model {mdl}...', end='', flush=True)\n",
    "        mdl.pull()\n",
    "        print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f24dbe-7eb2-4b47-97a9-e25db71eaa80",
   "metadata": {},
   "source": [
    "# Language models and their limitations\n",
    "\n",
    "We'll start a simple question that demonstrate that LLMs are eerily good at the pattern matching\n",
    "part of understanding, but might not be able to go beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1baa31f-f3d5-4dfc-a001-ec00e7a09fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f142b315-5190-47df-93ea-9e9d66ea94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        Okay, let's calculate 392373366237 * 562310951975.\n",
       "\n",
       "This is a large multiplication, and it's best done using a calculator or a programming language.  Here's the result:\n",
       "\n",
       "**2236358838888888888888888888888888888888\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content=\"Okay, let's calculate 392373366237 * 562310951975.\\n\\nThis is a large multiplication, and it's best done using a calculator or a programming language.  Here's the result:\\n\\n**2236358838888888888888888888888888888888\", additional_kwargs={}, response_metadata={}, id='lc_run--019b374f-6e3a-71f1-becf-cc95e394dbaa-0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        I apologize, but I cannot perform this calculation. The calculation tool I have access to is specialized for calculating mathematical operations related to financial calculations, such as calculating stock market valuations or stock market indices. I cannot access or perform complex financial data like this.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='I apologize, but I cannot perform this calculation. The calculation tool I have access to is specialized for calculating mathematical operations related to financial calculations, such as calculating stock market valuations or stock market indices. I cannot access or perform complex financial data like this.', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:52:32.893337884Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15383443989, 'load_duration': 1883625294, 'prompt_eval_count': 42, 'prompt_eval_duration': 4166037781, 'eval_count': 51, 'eval_duration': 9261176107, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b374f-d562-7010-ab33-45343303aabf-0', usage_metadata={'input_tokens': 42, 'output_tokens': 51, 'total_tokens': 93})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The result of multiplying 392373366237 by 562310951975 is:\n",
       "\n",
       "220,646,204,278,964,000,795\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The result of multiplying 392373366237 by 562310951975 is:\\n\\n220,646,204,278,964,000,795', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:52:43.390581719Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10459409991, 'load_duration': 5442385739, 'prompt_eval_count': 46, 'prompt_eval_duration': 719383588, 'eval_count': 31, 'eval_duration': 4278871024, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3750-11a1-7c22-be68-7bb258713622-0', usage_metadata={'input_tokens': 46, 'output_tokens': 31, 'total_tokens': 77})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage('Make the following calculation: 392373366237 * 562310951975')\n",
    "for mdl in MODELS:\n",
    "    print(f'\\n\\033[1m{str(mdl)}\\033[0m\\n')\n",
    "    chat = mdl.new_chat()\n",
    "    response = chat.invoke([human_message])\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524db586-53bb-4ec6-a706-9d6a6f19ae7c",
   "metadata": {},
   "source": [
    "\n",
    "Is this correct? The actual answer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55080cca-304b-4e1e-96ae-a8864c04fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220,635,841,098,362,793,468,075\n"
     ]
    }
   ],
   "source": [
    "print(f'{392373366237 * 562310951975:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501905b7-ff51-4e88-b8d8-9f99c7735f9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Unless this notebook is running in a future when LLMs have learned to multiply, or this notebook became part of the training data, the LLM answer is either likely not correct **OR** the LLM declines to provide an answer to the question (which is the case with `functiongemma`).\n",
    "\n",
    "One should not rely on the current LLM technology to perform correctly mathematical operations, or more generally to evaluate expressions or run algorithms. However, LLMs have seen training data that describes steps\n",
    "to resolve similar problems. They might be able to generate code (software instructions), for example a function call, to get to the answer. This can already be observed when using `gemma3:4b`: the answer from the LLM shows includes Python code that returns the correct answer is evaluated.\n",
    "\n",
    "If we have a way to know when LLM answers contains code, we could simply evaluate that code and consider the resulting text its answer a dialogue. In fact, it does not even have to be a dialogue and code evaluation represent simply an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ace3e-23a3-4407-bb6f-72ef29840109",
   "metadata": {},
   "source": [
    "Additional instructions in the prompt can get us there with this example. We can add to it that if evaluating code seems better path to the correct answer, only code should be returned. To facilitate the triaging of answers (answer is text / the direct answer *vs* answer is code to get to the answer), we can also indicate the format to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e271f4-d916-4e5d-a5b2-c56e8f50d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```python\n",
       "print(392373366237 * 562310951975)\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```python\\nprint(392373366237 * 562310951975)\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:53:08.93582477Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5602291748, 'load_duration': 134981785, 'prompt_eval_count': 83, 'prompt_eval_duration': 993722234, 'eval_count': 35, 'eval_duration': 4406832911, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3750-8863-7290-80ad-d368f2a215e3-0', usage_metadata={'input_tokens': 83, 'output_tokens': 35, 'total_tokens': 118})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        I apologize, but I cannot assist with generating Python code. My current capabilities are limited to assisting with mathematical calculations and data analysis using the provided tools. I cannot generate programming code.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='I apologize, but I cannot assist with generating Python code. My current capabilities are limited to assisting with mathematical calculations and data analysis using the provided tools. I cannot generate programming code.', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:53:11.951815318Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3000338835, 'load_duration': 226857731, 'prompt_eval_count': 87, 'prompt_eval_duration': 119024520, 'eval_count': 37, 'eval_duration': 2595421552, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3750-9e56-7ff3-b3dd-a8aceac0c091-0', usage_metadata={'input_tokens': 87, 'output_tokens': 37, 'total_tokens': 124})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```python\n",
       "result = 392373366237 * 562310951975\n",
       "result\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```python\\nresult = 392373366237 * 562310951975\\nresult\\n```', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:53:17.572282746Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5601954745, 'load_duration': 57452067, 'prompt_eval_count': 86, 'prompt_eval_duration': 3056007529, 'eval_count': 21, 'eval_duration': 2472484344, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3750-aa21-77a1-ad0b-a6962088e67f-0', usage_metadata={'input_tokens': 86, 'output_tokens': 21, 'total_tokens': 107})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_as_code = HumanMessage(\"\"\"\n",
    "IF this type of question would be better answered by evaluating code, return instead\n",
    "ONLY Python code in the answer. Use the Markdown format for code snippets:\n",
    "```python\n",
    "<code>\n",
    "```\n",
    "\"\"\")\n",
    "human_message = HumanMessage(\"\"\"Make the following calculation: 392373366237 * 562310951975\"\"\")\n",
    "for mdl in MODELS:\n",
    "    print(highlight(str(mdl)))\n",
    "    chat = mdl.new_chat()\n",
    "    response = chat.invoke([human_message, answer_as_code])\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ccb5c-bca6-4f9c-be94-7948dd4f95a4",
   "metadata": {},
   "source": [
    "This is looking promising, except for `functiongemma` that is adamant about being unabe to generate code.\n",
    "\n",
    "A good check to perform with LLMs is whether a new prompt causes regression (that is loose the ability to answer some questions correctly). To achieve this we ask a question that should be answerable without code.\n",
    "promptThat is a question for which the answer was almost certainly in the training data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f981aa2-9a7d-400b-94b2-97830d24d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```python\n",
       "print(\"The name of the natural satellite orbiting around the Earth is the Moon.\")\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```python\\nprint(\"The name of the natural satellite orbiting around the Earth is the Moon.\")\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:53:21.633500365Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4049036543, 'load_duration': 111993739, 'prompt_eval_count': 64, 'prompt_eval_duration': 1229417765, 'eval_count': 22, 'eval_duration': 2693744238, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3750-c00f-7c83-8f2f-e40518eec97d-0', usage_metadata={'input_tokens': 64, 'output_tokens': 22, 'total_tokens': 86})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        I apologize, but I cannot assist with programming or generating Python code. My current capabilities are limited to assisting with tasks related to natural language processing and text generation. I cannot provide programming advice or code snippets.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='I apologize, but I cannot assist with programming or generating Python code. My current capabilities are limited to assisting with tasks related to natural language processing and text generation. I cannot provide programming advice or code snippets.', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:53:30.877379511Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9227157974, 'load_duration': 383140829, 'prompt_eval_count': 68, 'prompt_eval_duration': 3534807275, 'eval_count': 42, 'eval_duration': 5278100625, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3750-cff1-7702-b363-1c2d78f4fc3b-0', usage_metadata={'input_tokens': 68, 'output_tokens': 42, 'total_tokens': 110})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The name of the natural satellite orbiting around the Earth is called **the Moon**.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The name of the natural satellite orbiting around the Earth is called **the Moon**.', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:53:37.719574764Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6823422072, 'load_duration': 75365876, 'prompt_eval_count': 83, 'prompt_eval_duration': 4088770527, 'eval_count': 19, 'eval_duration': 2645278208, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3750-f40f-7450-afb9-3ce197b6039d-0', usage_metadata={'input_tokens': 83, 'output_tokens': 19, 'total_tokens': 102})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage(\"\"\"What is the name of the natural satellite orbiting around the earth?\"\"\")\n",
    "for mdl in MODELS:\n",
    "    print(highlight(str(mdl)))\n",
    "    chat = mdl.new_chat()\n",
    "    response = chat.invoke([human_message, answer_as_code])\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba2038-fe69-4e53-8126-0d748129887e",
   "metadata": {},
   "source": [
    "While `gemma3:4b` its technically correct, it may indicate that this model will resort to writing code more often than necessary. `functiongemma` is consistent in messaging that it cannot generate code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae76940-5457-4ecf-818d-c67cf0f4627b",
   "metadata": {},
   "source": [
    "# Adding tools as context\n",
    "\n",
    "Agents are language models to which the availability of tools (software functions), with a description of what they do, is specified as context. For `gemma3:4b` this can be achieved through a system prompt, while a few other models with a \"tools\" capability Python functions can passed directly. This is the case with `granite4:1b` for example.\n",
    "\n",
    "## Tools in prompt\n",
    "\n",
    "We showed earlier that a prompt can indicate that code evaluation is available.\n",
    "Similarly, we can indicate that tools are available and calling a tool can be used as an answer.\n",
    "\n",
    "If we make our multiplication a tool, a prompt can look like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d8a6648-b39f-41fd-9a8d-6e1ae5a9f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_tools = \"\"\"\n",
    "You have access to functions. \n",
    "You don't have to use tools if not necessary, but *IF* you decide to invoke any of the function(s),\n",
    "you MUST put it in a valid JSON expression like:\n",
    "[\n",
    "  {\"name\": function name,\n",
    "   \"parameters\": dictionary of argument name and its value}\n",
    "]\n",
    "\n",
    "You SHOULD NOT include any other text in the response if you call a function.\n",
    "\n",
    "Functions:\n",
    "[\n",
    "  { \"name\": \"multiply\",\n",
    "    \"description\": \"Multiply two numbers.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "          \"a\": \"number\",\n",
    "          \"b\": \"number\"\n",
    "      },\n",
    "      \"required\": [\n",
    "          \"a\", \"b\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1fe2c-82ea-4845-bcb3-da20e46502f6",
   "metadata": {},
   "source": [
    "## Tools as `tools`\n",
    "\n",
    "The use of tools has been somewhat formalized since the early days of prompt engineering, a mere double-digit number\n",
    "of months in the past, we have seen so far.\n",
    "\n",
    "Many of the most recent language model specifically include a capability called \"tools\". This frees a user, or developer,\n",
    "from having to work on prompt sections that declare the availablity of tools in a way that the language model reacts to it, and on parsing out if an answer is text or code. `langchain` provides a wrapper for this and let one simply declare any Python function as tool. For the multiplication we can just plug the Python multiplication operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d06f06-7e0b-44c8-a003-a28e8e08e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "tools = [tool(operator.mul)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1b152-e72d-411e-83dc-9621bc585bfc",
   "metadata": {},
   "source": [
    "## Language model opting to answer with a tool call\n",
    "\n",
    "We have shown that a prompt context, wrapped in a `langchain` tool declaration when the model has the capability, can lead a language model to answer with code. If we ask again our multiplication question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0780b2-8460-40fc-a241-65a98db4b22d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"multiply\",\n",
       "    \"parameters\": {\n",
       "      \"a\": 392373366237,\n",
       "      \"b\": 562310951975\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"multiply\",\\n    \"parameters\": {\\n      \"a\": 392373366237,\\n      \"b\": 562310951975\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:56:17.201230892Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17443248812, 'load_duration': 154185821, 'prompt_eval_count': 224, 'prompt_eval_duration': 4023212770, 'eval_count': 69, 'eval_duration': 13217112473, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3753-398c-7a53-bc6f-8f7f040cd29f-0', usage_metadata={'input_tokens': 224, 'output_tokens': 69, 'total_tokens': 293})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'mul', 'args': {'a': 392373366237, 'b': 562310951975}, 'id': '7c9769d1-8531-4868-866f-3d00eb598097', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:56:20.54147123Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3211586801, 'load_duration': 267494815, 'prompt_eval_count': 126, 'prompt_eval_duration': 242371681, 'eval_count': 37, 'eval_duration': 2679888246, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3753-7e30-7b30-8ce7-165750feb86e-0', tool_calls=[{'name': 'mul', 'args': {'a': 392373366237, 'b': 562310951975}, 'id': '7c9769d1-8531-4868-866f-3d00eb598097', 'type': 'tool_call'}], usage_metadata={'input_tokens': 126, 'output_tokens': 37, 'total_tokens': 163})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'mul', 'args': {'a': '392373366237', 'b': '562310951975'}, 'id': '62be38a6-3878-4ad3-839d-31093d5d8d3e', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:56:30.374992769Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9785316390, 'load_duration': 82755867, 'prompt_eval_count': 199, 'prompt_eval_duration': 6080778373, 'eval_count': 31, 'eval_duration': 3601300486, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3753-8aec-77b2-a195-77f02da8f5d8-0', tool_calls=[{'name': 'mul', 'args': {'a': '392373366237', 'b': '562310951975'}, 'id': '62be38a6-3878-4ad3-839d-31093d5d8d3e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 199, 'output_tokens': 31, 'total_tokens': 230})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage(\"\"\"Make the following calculation: 392373366237 * 562310951975\"\"\")\n",
    "\n",
    "for model in MODELS:\n",
    "    chat = model.new_chat()\n",
    "    if model.has_tools():\n",
    "        print(f'{highlight(model.name)} (with tools)\\n')\n",
    "        chat = chat.bind_tools(tools)\n",
    "        messages = [human_message]\n",
    "    else:\n",
    "        print(f'{highlight(model.name)}\\n')\n",
    "        messages = [SystemMessage(system_prompt_tools),\n",
    "                    human_message]\n",
    "    response = chat.invoke(messages)\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3ab98-b60c-4512-aa20-ac58ebb2cade",
   "metadata": {},
   "source": [
    "All language models understand the assignment and should provide correct function calls. Interestingly, creating a function call is creating code which `functiongemma` is otherwise convinced it is unable to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33783eec-2d72-48f6-ba3c-6d8acf17f0a0",
   "metadata": {},
   "source": [
    "# Agentic linear chaining - an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd37e72-5ce8-42c6-bd45-078fb67f220c",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "We create a local database for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707c233c-6cd6-440c-8f62-3d42dd619f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "dbcon = sqlite3.connect(':memory:', check_same_thread=False)\n",
    "dbcon.execute(\"\"\"\n",
    "CREATE TABLE medication(\n",
    "ndc STRING PRIMARY KEY,\n",
    "common_name STRING,\n",
    "quantity INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "for values in (\n",
    "    ('ABCD-DEF', 'Aspirin', 3), ('ABCD-GHI', 'Aspirin', 5), ('GHIK-KLM', 'bacitracin', 10)\n",
    "):\n",
    "    dbcon.execute(\"\"\"\n",
    "    INSERT INTO medication(ndc, common_name, quantity) VALUES (?,?,?);\n",
    "    \"\"\", values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd787f-11f1-4a3b-b75b-5ca7a86712d0",
   "metadata": {},
   "source": [
    "## Tools to query a database\n",
    "\n",
    "Functions to query the database:\n",
    "- `get_ndc_code` to get all NDC codes for the common name of a medication\n",
    "- `get_medication_stock` to get the stock for a given NDC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fd751c7-b3aa-4510-baee-b14a523013db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndc_code(medication: str) -> tuple[str, ...]:\n",
    "    \"\"\"Get possible ndc codes for a medication.\n",
    "    \n",
    "    Args:\n",
    "      medication: name of the medication for which NDC code should be returned.\n",
    "    \"\"\"\n",
    "    cursor = dbcon.cursor()\n",
    "    cursor.execute('SELECT ndc FROM medication WHERE common_name==?',\n",
    "                   (medication, ))\n",
    "    return tuple(row[0] for row in cursor.fetchall())\n",
    "\n",
    "\n",
    "def get_ndc_stock(ndc_code: str) -> int:\n",
    "    \"\"\"Get the number of boxes of medication in stock.\n",
    "    \n",
    "    Args:\n",
    "      ndc_code: NDC code to retrieve stock for.\n",
    "    \"\"\"\n",
    "    cursor = dbcon.cursor()\n",
    "    cursor.execute('SELECT quantity FROM medication WHERE ndc==?',\n",
    "                   (ndc_code, ))\n",
    "    return cursor.fetchone()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c806bb0-ef29-404e-975d-b65bcd2a60f4",
   "metadata": {},
   "source": [
    "## Prompting an LLM to use tools (when needed)\n",
    "\n",
    "The prompt may contain several tools available. A prompt context will be similar to the one we showed for the multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2514d9eb-f292-4771-9db9-10c6558ca1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_tools = \"\"\"\n",
    "You have access to functions. \n",
    "You don't have to use tools if not necessary, but *IF* you decide to invoke any of the function(s),\n",
    "you MUST put it in a valid JSON expression like:\n",
    "[\n",
    "  {\"name\": function name,\n",
    "   \"parameters\": dictionary of argument name and its value}\n",
    "]\n",
    "\n",
    "You SHOULD NOT include any other text in the response if you call a function.\n",
    "\n",
    "Functions:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"get_ndc_stock\",\n",
    "    \"description\": \"Get the number of boxes of medication in stock.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"ndc_code\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"ndc_code\"\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"get_ndc_code\",\n",
    "    \"description\": \"Get possible ndc codes for a medication.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"medication\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"medication\"\n",
    "      ]\n",
    "    },\n",
    "  }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003ffb3-7718-440b-be9a-e15c67751761",
   "metadata": {},
   "source": [
    "Whenever a model has the capabilities \"tools\", the user or developer is spared the prompt context engineering.\n",
    "The decorator `tool()` in `langchain` can be used instead.\n",
    "\n",
    "Note: `langchain` is essentially an abstraction layer over prompts.\n",
    "It can create the equivalent of our engineered prompt contexts from Python function definitions,\n",
    "feeding the LLM with information in the docstring, or in additional declarative structures\n",
    "(see https://docs.langchain.com/oss/python/langchain/tools#advanced-schema-definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b8408f1-7664-4703-85c0-0b07099af717",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool(get_ndc_code), tool(get_ndc_stock)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8606c95c-9e26-418e-b3cd-bcd93886cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        January! \n",
       "\n",
       "Do you want to play a quick game of guessing months? ðŸ˜Š\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='January! \\n\\nDo you want to play a quick game of guessing months? ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:56:44.035293361Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3309531741, 'load_duration': 157166245, 'prompt_eval_count': 22, 'prompt_eval_duration': 474170429, 'eval_count': 18, 'eval_duration': 2671110580, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3753-d994-75d0-95c4-ac1ee09da8c9-0', usage_metadata={'input_tokens': 22, 'output_tokens': 18, 'total_tokens': 40})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        I apologize, but I cannot assist with retrieving the specific NDC code for the first month of the year. My current tools are designed for retrieving medication-related data and stock information. I cannot query or retrieve specific calendar or stock data for specific dates.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='I apologize, but I cannot assist with retrieving the specific NDC code for the first month of the year. My current tools are designed for retrieving medication-related data and stock information. I cannot query or retrieve specific calendar or stock data for specific dates.', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:56:56.763608842Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12611970214, 'load_duration': 288091059, 'prompt_eval_count': 187, 'prompt_eval_duration': 4427416337, 'eval_count': 51, 'eval_duration': 7846284522, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3753-e6f6-70a1-bffd-6fc74374d490-0', usage_metadata={'input_tokens': 187, 'output_tokens': 51, 'total_tokens': 238})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The first month of the year is **January**.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The first month of the year is **January**.', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:57:04.964227029Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8139267110, 'load_duration': 67773414, 'prompt_eval_count': 268, 'prompt_eval_duration': 6357916317, 'eval_count': 12, 'eval_duration': 1695143557, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3754-1877-7431-a665-d9602c0eeaaa-0', usage_metadata={'input_tokens': 268, 'output_tokens': 12, 'total_tokens': 280})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def invoke_model(model, human_message, system_message=SystemMessage('')):\n",
    "    chat = model.new_chat()\n",
    "    if model.has_tools():\n",
    "        print(f'\\n{highlight(model.name)} (with tools)\\n')\n",
    "        chat = chat.bind_tools(tools)\n",
    "    else:\n",
    "        print(f'\\n{highlight(model.name)}\\n')\n",
    "    messages = [system_message,\n",
    "                human_message]\n",
    "    response = chat.invoke(messages)\n",
    "    return response, chat\n",
    "\n",
    "human_message = HumanMessage('Name the first month of the year.')\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message)\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2ecfb-8c5d-4ddc-8fcf-537c098c7f04",
   "metadata": {},
   "source": [
    "`functiongemma:270` appears challenged with the question. Its size (just over 1/4th of `granite4:1b`) might be the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaac153b-042f-49b7-b6a1-4dca1f1a897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_map = {tool.name: tool for tool in tools}\n",
    "if len(tool_map) != len(tools):\n",
    "    raise ValueError(\"'tools' contains at lease one tool name duplicate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b89ee6be-12f2-446a-a92e-ff451dee26da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"get_ndc_stock\",\n",
       "    \"parameters\": {\n",
       "      \"ndc_code\": \"ABCD-DEF\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"get_ndc_stock\",\\n    \"parameters\": {\\n      \"ndc_code\": \"ABCD-DEF\"\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:57:22.561638059Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17479447897, 'load_duration': 171918526, 'prompt_eval_count': 318, 'prompt_eval_duration': 5923119520, 'eval_count': 50, 'eval_duration': 11354512648, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3754-38b9-79c2-bf49-feebe189954e-0', usage_metadata={'input_tokens': 318, 'output_tokens': 50, 'total_tokens': 368})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing automatically if there is a tool call is left as an exercise for the reader.\n",
      "\n",
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': 'a63df468-8c35-4d1c-ba01-f07c58b4a29a', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:57:30.780728734Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8043054729, 'load_duration': 245270405, 'prompt_eval_count': 483, 'prompt_eval_duration': 6881853573, 'eval_count': 23, 'eval_duration': 903915132, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3754-7db0-7070-85c9-03324e9cf855-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': 'a63df468-8c35-4d1c-ba01-f07c58b4a29a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 483, 'output_tokens': 23, 'total_tokens': 506})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> calling get_ndc_stock(ndc_code='ABCD-DEF')\n",
      "      result:\n",
      "        3\n",
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': 'fe96a455-02c4-4f10-ab45-e64a4d3ff059', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:57:50.134461636Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19305905438, 'load_duration': 53794011, 'prompt_eval_count': 518, 'prompt_eval_duration': 14909120159, 'eval_count': 27, 'eval_duration': 4311319780, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3754-9d4b-7ee0-a582-f89a70cdfa89-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': 'fe96a455-02c4-4f10-ab45-e64a4d3ff059', 'type': 'tool_call'}], usage_metadata={'input_tokens': 518, 'output_tokens': 27, 'total_tokens': 545})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> calling get_ndc_stock(ndc_code='ABCD-DEF')\n",
      "      result:\n",
      "        3\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage('How many boxes of medication with NDC code ABCD-DEF do we have left?')\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message, SystemMessage(system_prompt_tools))\n",
    "    display(response)\n",
    "    if response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            print(\n",
    "                f'  --> calling {tool_call[\"name\"]}'\n",
    "                f'({\", \".join(\"=\".join((k, repr(v))) for k, v in tool_call[\"args\"].items())})'\n",
    "            )\n",
    "            selected_tool = tool_map[tool_call['name']]\n",
    "            res = selected_tool.invoke(tool_call)\n",
    "            print('      result:')\n",
    "            print(f'        {res.content}')\n",
    "    else:\n",
    "        print('Assessing automatically if there is a tool call is left as an exercise for the reader.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d949565-bb5d-4e3a-a70b-2e5b2ef8d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"get_ndc_code\",\n",
       "    \"parameters\": {\n",
       "      \"medication\": \"Aspirin\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"get_ndc_code\",\\n    \"parameters\": {\\n      \"medication\": \"Aspirin\"\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:58:00.993286466Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10735424984, 'load_duration': 187402978, 'prompt_eval_count': 315, 'prompt_eval_duration': 952424576, 'eval_count': 48, 'eval_duration': 9541583935, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3754-e930-79c1-aec7-596a87b23c76-0', usage_metadata={'input_tokens': 315, 'output_tokens': 48, 'total_tokens': 363})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing automatically if there is a tool call is left as an exercise for the reader.\n",
      "\n",
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '5d987bf2-cc49-4058-b0d7-08196877a7d3', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:58:06.693394285Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5520482910, 'load_duration': 284049169, 'prompt_eval_count': 480, 'prompt_eval_duration': 4023532210, 'eval_count': 21, 'eval_duration': 1176127051, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3755-13d3-7130-9404-230265a62b9e-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '5d987bf2-cc49-4058-b0d7-08196877a7d3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 480, 'output_tokens': 21, 'total_tokens': 501})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      result:\n",
      "        [\"ABCD-DEF\", \"ABCD-GHI\"]\n",
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '65211877-5146-4db6-acfd-6c658edb630b', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:58:11.403304958Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4647825876, 'load_duration': 48513311, 'prompt_eval_count': 514, 'prompt_eval_duration': 624487429, 'eval_count': 25, 'eval_duration': 3947049016, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3755-29a2-7be1-b164-a298ea42b847-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '65211877-5146-4db6-acfd-6c658edb630b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 514, 'output_tokens': 25, 'total_tokens': 539})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      result:\n",
      "        [\"ABCD-DEF\", \"ABCD-GHI\"]\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(\"What are possible NDC codes for the medication 'Aspirin'?\")\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message, SystemMessage(system_prompt_tools))\n",
    "    display(response)\n",
    "    if response.tool_calls:\n",
    "        print('      result:')\n",
    "        for tool_call in response.tool_calls:\n",
    "            selected_tool = tool_map[tool_call['name']]\n",
    "            res = selected_tool.invoke(tool_call)    \n",
    "            print(f'        {res.content}')\n",
    "    else:\n",
    "        print('Assessing automatically if there is a tool call is left as an exercise for the reader.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e68e5-a09f-4778-915d-e206dc823708",
   "metadata": {},
   "source": [
    "Cool. But what did we exactly gain here? If you are are programmer or a data analyst knowing how to write SQL, arguably not much. However, what we have achieved is akin to a text user interface, or language user interface, that can be an alternative to a graphical user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0e438-ae36-40ef-bc90-d81d2ba431bf",
   "metadata": {},
   "source": [
    "## Reasoning in steps\n",
    "\n",
    "So far we only showed questions for which either the language model had the answer memorized from the data it was trained on, or for which the answer was one tool call away.\n",
    "\n",
    "There are questions questions for which the answer requires a sequence of recall from training data or tool calls, with the output of a step used as the input to the next step. For example, asking for how many Aspirin boxes are in stock, would require to 1) get the NDC codes for Aspirin products, 2) query the stock for each, and finally 3) sum those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b26b128-758a-4a14-872a-f222dfc4b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        ```json\n",
       "[\n",
       "  {\n",
       "    \"name\": \"get_ndc_stock\",\n",
       "    \"parameters\": {\n",
       "      \"ndc_code\": \"98304-600\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='```json\\n[\\n  {\\n    \"name\": \"get_ndc_stock\",\\n    \"parameters\": {\\n      \"ndc_code\": \"98304-600\"\\n    }\\n  }\\n]\\n```', additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-12-19T15:58:36.459900933Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9145060624, 'load_duration': 161680650, 'prompt_eval_count': 315, 'prompt_eval_duration': 503586310, 'eval_count': 56, 'eval_duration': 8434434702, 'logprobs': None, 'model_name': 'gemma3:4b', 'model_provider': 'ollama'}, id='lc_run--019b3755-79f1-7ed1-b621-80a21c777557-0', usage_metadata={'input_tokens': 315, 'output_tokens': 56, 'total_tokens': 371})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'Aspirin'}, 'id': '7d0683fa-c668-4640-86c8-2b74bc4f273f', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T15:58:38.843606411Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2151825106, 'load_duration': 387322588, 'prompt_eval_count': 480, 'prompt_eval_duration': 243721144, 'eval_count': 23, 'eval_duration': 1501447963, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3755-9e8b-7672-b0f7-f907ff14bc50-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'Aspirin'}, 'id': '7d0683fa-c668-4640-86c8-2b74bc4f273f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 480, 'output_tokens': 23, 'total_tokens': 503})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m (with tools)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': 'bf98a6fe-bc7f-4027-ae38-1a8e8a9b8c2e', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T15:58:46.687847406Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7795167505, 'load_duration': 52036147, 'prompt_eval_count': 514, 'prompt_eval_duration': 4347833691, 'eval_count': 25, 'eval_duration': 3366442035, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3755-a72b-7521-849a-744947dfc784-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': 'bf98a6fe-bc7f-4027-ae38-1a8e8a9b8c2e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 514, 'output_tokens': 25, 'total_tokens': 539})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_message = HumanMessage('How many boxes of Aspirin do we have in stock altogether?')\n",
    "\n",
    "for model in MODELS:\n",
    "    response, chat = invoke_model(model, human_message, SystemMessage(system_prompt_tools))\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e3230-52f6-4f48-a71e-e1fdc745a1ce",
   "metadata": {},
   "source": [
    "Odds are are that the results are not great. Bare language models used in this notebook generally do not handle this well without guidance from a prompt. For example,  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199acc6-4d4e-4f41-b8f3-1c9fd11c6695",
   "metadata": {},
   "source": [
    "## ReAct\n",
    "\n",
    "The ReAct approach prompts a language model to match its answers to the following pattern:\n",
    "1. **question**\n",
    "2. **thought:** the language model's answer to the question. The answer can be the use of a tool.\n",
    "3. **action:** whenever the answer is the use of a tool, a function call, this is selected here.\n",
    "4. **action input:** the input to the action.\n",
    "5. **observation:** observe the result of performing the action.\n",
    "6. The sequence of steps 2 to 5 can be repeated N times\n",
    "7. **final answer**\n",
    "\n",
    "It is derived from the empirical observation that LLMs appeared to give more \"thoughtful\" answers, or better solve reasoning-oriented questions when asked to think step by step.\n",
    "\n",
    "React can be idependently achieved through adding the description to a prompt and iteratively parsing language model responses and crafting new messages. However, in many scenario this is can be abstracted out through the use of library. We use `langchain` again to demonstrate how it can work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f87632a-ccf4-444c-b1e3-f3ca03557058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mgemma3:4b\u001b[0m\n",
      "Parsing AI messages into next actions and results is left as an exercise.\n",
      "\n",
      "\u001b[1mfunctiongemma:270m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'Aspirin'}, 'id': '0d55848c-3003-408c-846e-2ccb2238bc38', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:270m', 'created_at': '2025-12-19T16:15:15.100583755Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1757541174, 'load_duration': 139784081, 'prompt_eval_count': 192, 'prompt_eval_duration': 6755575, 'eval_count': 23, 'eval_duration': 1597907890, 'logprobs': None, 'model_name': 'functiongemma:270m', 'model_provider': 'ollama'}, id='lc_run--019b3764-d3bd-7780-a4dc-f45106de7676-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'Aspirin'}, 'id': '0d55848c-3003-408c-846e-2ccb2238bc38', 'type': 'tool_call'}], usage_metadata={'input_tokens': 192, 'output_tokens': 23, 'total_tokens': 215})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "'NoneType' object is not subscriptable\n",
      "\n",
      "\u001b[1mgranite4:1b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '89afdd0e-92bf-4d6e-9270-10ec8dfb20c9', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T16:15:25.468328729Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10289600373, 'load_duration': 3945861777, 'prompt_eval_count': 274, 'prompt_eval_duration': 3503917652, 'eval_count': 25, 'eval_duration': 2819616871, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3764-dae9-7263-bf54-e82336f4d328-0', tool_calls=[{'name': 'get_ndc_code', 'args': {'medication': 'Aspirin'}, 'id': '89afdd0e-92bf-4d6e-9270-10ec8dfb20c9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 274, 'output_tokens': 25, 'total_tokens': 299})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
       "        <tbody>\n",
       "        <tr>\n",
       "          \n",
       "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "          <td style=\"text-align: left; text-indent: 0px;\">\n",
       "            <div style=\"white-space: pre-wrap;\">\n",
       "            [\"ABCD-DEF\", \"ABCD-GHI\"]\n",
       "            </div>\n",
       "          </td>\n",
       "          \n",
       "        </tr>\n",
       "        </tbody></table>\n",
       "        "
      ],
      "text/plain": [
       "ToolMessage(content='[\"ABCD-DEF\", \"ABCD-GHI\"]', name='get_ndc_code', id='d8cc0dd6-bb1f-4322-a0f0-911b83bc8221', tool_call_id='89afdd0e-92bf-4d6e-9270-10ec8dfb20c9')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': '6886023e-3e2e-4ea3-a0f4-5e0a4e1b0c6a', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T16:15:29.79522246Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4318452056, 'load_duration': 53368600, 'prompt_eval_count': 323, 'prompt_eval_duration': 669659728, 'eval_count': 27, 'eval_duration': 3576577944, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3765-0323-7931-9608-2cc0f0d1aaab-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-DEF'}, 'id': '6886023e-3e2e-4ea3-a0f4-5e0a4e1b0c6a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 323, 'output_tokens': 27, 'total_tokens': 350})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
       "        <tbody>\n",
       "        <tr>\n",
       "          \n",
       "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "          <td style=\"text-align: left; text-indent: 0px;\">\n",
       "            <div style=\"white-space: pre-wrap;\">\n",
       "            3\n",
       "            </div>\n",
       "          </td>\n",
       "          \n",
       "        </tr>\n",
       "        </tbody></table>\n",
       "        "
      ],
      "text/plain": [
       "ToolMessage(content='3', name='get_ndc_stock', id='63d75cad-c063-4d81-9e2e-294e0656cd99', tool_call_id='6886023e-3e2e-4ea3-a0f4-5e0a4e1b0c6a')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(150, 40, 40); width: 6em;\"><b>tools:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        [{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-GHI'}, 'id': '7e28b8c2-e435-4474-86ef-e55bc82bc785', 'type': 'tool_call'}]\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T16:15:33.78495174Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3981372369, 'load_duration': 30995291, 'prompt_eval_count': 364, 'prompt_eval_duration': 365822022, 'eval_count': 26, 'eval_duration': 3565190022, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3765-140a-74f1-ab9c-4c5dbd4fc052-0', tool_calls=[{'name': 'get_ndc_stock', 'args': {'ndc_code': 'ABCD-GHI'}, 'id': '7e28b8c2-e435-4474-86ef-e55bc82bc785', 'type': 'tool_call'}], usage_metadata={'input_tokens': 364, 'output_tokens': 26, 'total_tokens': 390})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead><tr><td colspan=\"2\">Tool message</td></tr></thead>\n",
       "        <tbody>\n",
       "        <tr>\n",
       "          \n",
       "          <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "          <td style=\"text-align: left; text-indent: 0px;\">\n",
       "            <div style=\"white-space: pre-wrap;\">\n",
       "            5\n",
       "            </div>\n",
       "          </td>\n",
       "          \n",
       "        </tr>\n",
       "        </tbody></table>\n",
       "        "
      ],
      "text/plain": [
       "ToolMessage(content='5', name='get_ndc_stock', id='99690172-b124-451b-832f-3432de7a99fe', tool_call_id='7e28b8c2-e435-4474-86ef-e55bc82bc785')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><td colspan=\"2\">AI message</td></tr></thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      \n",
       "      <td style=\"vertical-align: top; border-right: 2px solid rgb(40, 40, 150); width: 6em;\"><b>response:</b></td>\n",
       "      <td style=\"text-align: left; text-indent: 0px;\">\n",
       "        <div style=\"white-space: pre-wrap;\">\n",
       "        The total number of boxes in stock for Aspirin is **8**.\n",
       "        </div>\n",
       "      </td>\n",
       "      \n",
       "      \n",
       "  </tr></tbody></table>\n",
       "  "
      ],
      "text/plain": [
       "AIMessage(content='The total number of boxes in stock for Aspirin is **8**.', additional_kwargs={}, response_metadata={'model': 'granite4:1b', 'created_at': '2025-12-19T16:15:36.729298734Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2936094184, 'load_duration': 49801926, 'prompt_eval_count': 405, 'prompt_eval_duration': 267335280, 'eval_count': 17, 'eval_duration': 2603902041, 'logprobs': None, 'model_name': 'granite4:1b', 'model_provider': 'ollama'}, id='lc_run--019b3765-23a0-7241-aa3e-7453857123e2-0', usage_metadata={'input_tokens': 405, 'output_tokens': 17, 'total_tokens': 422})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "human_message = HumanMessage(\"How many boxes of Aspirin do we have in stock altogether?\")\n",
    "\n",
    "for model in MODELS:\n",
    "    print(highlight(model.name))\n",
    "    if model.has_tools():\n",
    "        react = create_agent(model.new_chat(), tools=tools)\n",
    "        r_stream = react.stream({'messages': [human_message]}, stream_mode='updates')\n",
    "        try:\n",
    "            for chunk in r_stream:\n",
    "                for k in chunk.keys():\n",
    "                    for m in chunk[k]['messages']:\n",
    "                        display(m)\n",
    "        except TypeError as err:\n",
    "            print('Error:')\n",
    "            print(err)\n",
    "    else:\n",
    "        print('Parsing AI messages into next actions and results is left as an exercise.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e42bb7-e3b0-4079-b4d3-925092c86a53",
   "metadata": {},
   "source": [
    "When calling tools works without error, the answer is indeed correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af9ef0-cfc6-4df9-8520-b8ad36f107da",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Small language models are able to handle simple queries and chain sequences of steps to reach an answer. They can run locally / isolated and on relatively modest hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
